# -*- coding: utf-8 -*-
"""Task_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bvuEaiEfwuXEWj9V1Wll7YTJoWav5TCM

# **Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression , Lasso , Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score , root_mean_squared_error

"""# **Explanation**

x -> features : x_train , x_test
y_actual -> target : y_train , y_test
m -> number of samples (rows)

y_prediction -> target -> prediction value

w -> weight
b -> bias

Equation of linear regression : w * x + b

y_prediction = w * x + b

error = y_actual - y_prediction


cost_function :

  for each method we implement it on multiple rows (samples) of data using for loop

        1. MSE (Mean Squared Error)
                                    equation -> (error ** 2)
                                    cost function = (1 / 2m) * sum( (y_actual - y_prediction) ** 2 )


        2. MAE (Mean Absolute Error)
                                    equation -> abs(error)
                                    cost function = (1 / m) * sum( abs(y_actual - y_prediction) )


        3. RMSE (Root Mean Squared Error)
                                    equation -> sqrt( (error ** 2) )
                                    cost function = sqrt(  (1 / m) * sum( (y_actual - y_prediction) ** 2 ) )  )


        4. R2 (R-squared)
                                    equation -> 1 - (error / (y_actual - y_mean))
                                    y_mean = total_sum(y_actual) / m
                                    cost function = 1 - (  sum( (y_actual - y_prediction) ** 2 ) / sum( (y_actual - y_mean) ** 2 ) )  )


gradients computing :

  calculate derivate of cost function wrt w and b for each y_prediction value
  with w : multiple with x_train
  with b : multiple with 1
  do not forget the (1/m) to get the average

  for each method we implement it on multiple rows (samples) of data using for loop
  there is negative cuz -> error = y_actual - y_prediction = y_actual - (w * x + b) = y_actual - w * x - b
  and we diffrentiate wrt w and b

  derivative = sum( y_actual - y_prediction )

        1. without regularization
                                    dj_dw + = (-1 / m) * derivative * x_train
                                    dj_db + = (-1 / m) * derivative

        2. with regularization

              Regularization entirely solves the problem of overfitting, but λ is what determines the strength of this solution.
              It’s like a 'control knob' that makes the model either learn more details or stay simpler and generalize better.
              so the more lambda value is the less the model complex is.

              constant = (lambda_/(2*m))

                  1. l1 : lasso -> constant * sum( abs(weights) )
                      lambda_ makes the weights = 0   :->   feature selection
                                                regularization = constant * sum( abs(w) + abs(b) )
                                                dj_dw + = (-1 / m) * derivative * x_train + regularization
                                                dj_db + = (-1 / m) * derivative + regularization


                  2. l2 : ridge -> constant * sum( weights ** 2 )
                    lambda_ makes the weights very small   :->   shrinkage
                                                regularization = constant * sum(  ( w ** 2 ) + ( b ** 2 )  )
                                                dj_dw + = (-1 / m) * derivative * x_train + regularization
                                                dj_db + = (-1 / m) * derivative + regularization



updating the weights :

  we update the weights to try to get the minimum loss.
  but it is not enought to calculate the gradients only.
  we have to multiple it with learning_rate.
  learning rate determines the step size in the weight update.

  when choosing small value of learning rate , we increase the number of iterations , higher computational expensive.

  when choosing large value of learning rate , it may end with overshooting , the model go far away from the globel minimum
  do the model might not get the minimum cost loss at all!

  after choosing good value of learning_rate and multiple it with the gradients,
  we update the weights and biases.

  we subtract the gradients from the old weights and biases cuz :
    in the loss vs weights graph , we see that
    the model is in the left or the right side of the global minimum value

        - if the model is in the left side of global minimum value , dj_dw -> -ve , dj_db -> -ve
            so w_new > w_old : now the model will move right

        - if the model is in the right side of global minimum value , dj_dw -> +ve , dj_db -> +ve
            so w_new < w_old : now the model will move left

                          w_new = w_old - learning_rate * dj_dw
                          b_new = b_old - learning_rate * dj_db

"

# **Manual Implementation**
"""

def fun_linear_regression(x , w , b):
  if np.isscalar(w):
    Y_prediction = np.sum(w * x, axis=1) + b
  else:
    Y_prediction =  np.dot(x, w) + b

  return Y_prediction

def cost_function(Y_prediction , Y_train , m , fun_type = 'MSE'):
  total_cost = 0
  if fun_type == 'MSE':
    cost = (Y_prediction - Y_train) ** 2
    total_cost = total_cost + cost

    total_cost = total_cost * (1/(2*m))

  elif fun_type == 'MAE':
    cost = abs(Y_prediction - Y_train)
    total_cost = total_cost + cost

    total_cost = total_cost * (1/m)

  elif fun_type == 'RMSE':
    cost = (Y_prediction - Y_train) ** 2
    total_cost = total_cost + cost

    total_cost = np.sqrt(total_cost * (1/m))


  elif fun_type == 'r2':
    equation1_cost = 0
    equation2_mean = 0
    equation1_cost = equation1_cost + ((Y_prediction - Y_train) ** 2)
    equation2_mean = equation2_mean + ((Y_prediction - Y_train) ** 2)

    total_cost = 1 - (equation1_cost / equation2_mean)

  return total_cost

def regularization(w , b , m , lambda_ , reg_type = 'l1'):

  regularization_term = 0
  constant = (lambda_/(2*m))

  if reg_type == 'l1':
    weights = abs(w)  + abs(b)
  elif reg_type == 'l2':
    weights = (w**2 + b**2)
  else:
    raise ValueError("Invalid regularization type. Use 'l1' or 'l2'.")

    regularization_term = regularization_term + weights

  regularization_term = constant * regularization_term

  return regularization_term

def gradient_computing(Y_prediction , Y_train , X_train , m , w = None , b = None , with_or_without_reg = 'without' , lambda_ = None):
  dj_dw = 0
  dj_db = 0
  error = (Y_train - Y_prediction)

  dj_dw = (-1 * np.sum(np.dot(X_train.T, error)))
  dj_db = (-1 * np.sum(error))

  dj_dw = dj_dw * (1/m)
  dj_db = dj_db * (1/m)


  if with_or_without_reg == 'with-l1':
    regularization_term = regularization(w , b , m , lambda_ , 'l1')

    dj_dw = dj_dw + regularization_term
    dj_db = dj_db + regularization_term


  elif  with_or_without_reg == 'with-l2':
    regularization_term = regularization(w , b , m , lambda_ , 'l2')

    dj_dw = dj_dw + regularization_term
    dj_db = dj_db + regularization_term

  return dj_dw , dj_db

def update(w , b , alpha , dj_dw , dj_db):
  w_new = w - alpha * dj_dw
  b_new = b - alpha * dj_db

  return w_new , b_new

def model(X_train , Y_train , w , b , alpha , cost_function_type , iter):
  j_history = []
  w_history = []
  b_history = []
  m = X_train.shape[0]
  for i in range(iter):

    Y_prediction = fun_linear_regression(X_train , w , b)

    total_cost = cost_function(Y_prediction , Y_train , m , cost_function_type)
    j_history.append(total_cost)

    dj_dw , dj_db = gradient_computing(Y_prediction , Y_train , X_train , m , w = None , b = None , with_or_without_reg = 'without' , lambda_ = None)

    w , b = update(w , b , alpha , dj_dw , dj_db)
    w_history.append(w)
    b_history.append(b)

  print('without regularization')
  print(f'iteration {iter}: Total cost = {total_cost}, w = {w}, b = {b}' , end='\n\n')

  print(end = '\n\n\n')

  return w , b , j_history , w_history , b_history

def model_l1(X_train , Y_train , w , b , alpha , cost_function_type , lambda_ , iter):
  j_history = []
  w_history = []
  b_history = []
  m = X_train.shape[0]
  for i in range(iter):

    Y_prediction = fun_linear_regression(X_train , w , b)

    total_cost = cost_function(Y_prediction , Y_train , m , cost_function_type)
    j_history.append(total_cost)

    dj_dw , dj_db = gradient_computing(Y_prediction , Y_train , X_train , m , w = w , b = b , with_or_without_reg = 'with-l1' , lambda_ = lambda_)

    w , b = update(w , b , alpha , dj_dw , dj_db)
    w_history.append(w)
    b_history.append(b)

  print('with lasso regularization')
  print(f'iteration {iter}: Total cost = {total_cost}, w = {w}, b = {b}' , end='\n\n')

  print(end = '\n\n\n')

  return w , b , j_history , w_history , b_history

def model_l2(X_train , Y_train , w , b , alpha , cost_function_type , lambda_ , iter):
  j_history = []
  w_history = []
  b_history = []
  m = X_train.shape[0]
  for i in range(iter):

    Y_prediction = fun_linear_regression(X_train , w , b)

    total_cost = cost_function(Y_prediction , Y_train , m , cost_function_type)
    j_history.append(total_cost)

    dj_dw , dj_db = gradient_computing(Y_prediction , Y_train , X_train , m , w = w , b = b , with_or_without_reg = 'with-l2' , lambda_ = lambda_)

    w , b = update(w , b , alpha , dj_dw , dj_db)
    w_history.append(w)
    b_history.append(b)

  print('with ridge regularization')
  print(f'iteration {iter}: Total cost = {total_cost}, w = {w}, b = {b}' , end='\n\n')

  print(end = '\n\n\n')

  return w , b , j_history , w_history , b_history

"""# **Example**"""

X_train = np.array([
                    [1,2,3,4,5],
                    [2,3,4,5,6],
                    [3,4,5,6,7],
                    [4,5,6,7,8],
                    [5,6,7,8,9]
                    ])

Y_train = np.array([300, 500 , 600 , 100 , 700])

for i in range(X_train.shape[1]):
  plt.scatter(X_train[:, i], Y_train, label=f'Feature {i+1}')
plt.title("For All Feature")
plt.xlabel("Feature values")
plt.ylabel("Target Y")
plt.legend()
plt.show()

# print(f'shape of X_train and Y_train: {X_train.shape}')
# plt.scatter(X_train, Y_train)
# plt.plot(X_train, Y_train, c = "r")
# plt.show()

for i in (range(X_train.shape[1])):
  plt.scatter(X_train[:,i], Y_train, label="Data")
  plt.plot(X_train[:,i], Y_train, c="r", label="Line")
  plt.title(f"Feature ({i + 1}) VS Target")
  plt.xlabel(f"Feature ({i + 1})")
  plt.ylabel("Y")
  plt.legend()
  plt.show()

m = m = X_train.shape[0] # number of samples
Y_prediction = np.zeros(X_train.shape) # target -> prediction value

w = 0 # weight
b = 0 # bias
Y_prediction = fun_linear_regression(X_train, w, b)

print(Y_prediction)

for i in range(X_train.shape[1]):
  plt.scatter(X_train[:, i], Y_prediction, label=f'Feature {i+1}')
plt.title("For All Feature")
plt.xlabel("Feature values")
plt.ylabel("Prediction Y")
plt.legend()
plt.show()

# plt.scatter(X_train, Y_train)
# plt.plot(X_train, Y_prediction, c = "r")
# plt.show()

total_cost_mse = cost_function(Y_prediction , Y_train , m , 'MSE')
print('MSE = ' + str(total_cost_mse) , end = '\n\n')

total_cost_mae = cost_function(Y_prediction , Y_train , m , 'MAE')
print('MAE = ' + str(total_cost_mae) , end= '\n\n')

total_cost_rmse = cost_function(Y_prediction , Y_train , m , 'RMSE')
print('RMSE = ' + str(total_cost_rmse) , end = '\n\n')

total_cost_r2 = cost_function(Y_prediction , Y_train , m , 'r2')
print('R2 = ' + str(total_cost_r2) , end = '\n\n')

regularization_term_l1 = regularization(w , b , m , 0.01 , 'l1')
print(f"Regularization Term: {regularization_term_l1}")

regularization_term_l2 = regularization(w , b , m , 0.01 , 'l2')
print(f"Regularization Term: {regularization_term_l2}")

dj_dw , dj_db = gradient_computing(Y_prediction , Y_train , X_train , m)
print(f"Derivative of cost function for w: {dj_dw}")
print(f"Derivative of cost function for b: {dj_db}" , end = '\n\n')

dj_dw_l1 , dj_db_l1 = gradient_computing(Y_prediction , Y_train , X_train , m , w , b , 'with-l1' , 0.01)
print(f"Derivative of cost function for w: {dj_dw_l1}")
print(f"Derivative of cost function for b: {dj_db_l1}" , end = '\n\n')

dj_dw_l2 , dj_db_l2 = gradient_computing(Y_prediction , Y_train , X_train , m , w , b , 'with-l2' , 0.01)
print(f"Derivative of cost function for w: {dj_dw_l2}")
print(f"Derivative of cost function for b: {dj_db_l2}" , end = '\n\n')

print(f"new w , b: {update(w , b , 0.01 , dj_dw , dj_db)}" )

"""# **Data Analyzing , Cleaning , Preprocessing , Spliting , ...**"""

data_frame = pd.read_csv("houses.txt", delimiter=",")

data_frame.shape

data_frame.info()

data_frame.describe()

for col in data_frame.columns:
  print(f"{col} \n Number of unique values {data_frame[col].nunique()} \n Unique values : \n {data_frame[col].unique()}" , end = '\n\n')

data_frame['6.500000000000000000e+01'] = data_frame['6.500000000000000000e+01'].astype(int)
data_frame['1.000000000000000000e+00'] = data_frame['1.000000000000000000e+00'].astype(int)
data_frame['2.000000000000000000e+00'] = data_frame['2.000000000000000000e+00'].astype(int)
data_frame['9.520000000000000000e+02'] = data_frame['9.520000000000000000e+02'].astype(int)

data_frame.info()

features = data_frame.iloc[:, :-1]
target = data_frame.iloc[:, -1]

print(f"Features Shape \n : {features.shape}")
print(f"Target Shape \n : {target.shape}")

print(f"Features \n : {features}")
# print(f"Target \n : {target}"

x_train , x_test , y_train , y_test = train_test_split(features , target , test_size=0.2 , random_state = 42)

print(x_train)

# columns = ['6.500000000000000000e+01' , '9.520000000000000000e+02']
# scaler = StandardScaler()
# for col in columns:
#   x_train[col] = scaler.fit_transform(x_train[[col]])
#   x_test[col] = scaler.fit_transform(x_test[[col]])

# x_train['6.500000000000000000e+01']

"""# **Task**"""

for col in x_train.columns:
  plt.scatter(x_train[col], y_train, label=f'{col}')
plt.title("For All Feature")
plt.xlabel("Feature values")
plt.ylabel("Target Y")
plt.legend()
plt.show()

w = 10
b = 10
alpha = 0.1
cost_function_type = 'MSE'
iter = 10

w_new , b_new , j_history , w_history , b_history = model(x_train , y_train , w , b , alpha , cost_function_type , iter)

m = m = x_train.shape[0] # number of samples
Y_prediction = np.zeros(x_train.shape) # target -> prediction value

Y_prediction = fun_linear_regression(x_train, w_new, b_new)

print(Y_prediction)

for col in x_train.columns:
  plt.scatter(x_train[col], Y_prediction, label=f'{col}')
plt.title("For All Feature")
plt.xlabel("Feature values")
plt.ylabel("Prediction Y")
plt.legend()
plt.show()

# plt.scatter(X_train, Y_train)
# plt.plot(X_train, Y_prediction, c = "r")
# plt.show()

error = y_train - Y_prediction
error

total_cost = cost_function(Y_prediction , y_train , m , 'r2')
total_cost

"""# **Task with Scikit-Learn**"""

linear_regression_model = LinearRegression()
lasso_model = Lasso(alpha = 0.1)
ridge_model = Ridge(alpha = 0.1)

linear_regression_model.fit(x_train , y_train)
lasso_model.fit(x_train , y_train)
ridge_model.fit(x_train , y_train)

y_pred_train_lr = linear_regression_model.predict(x_train)
y_pred_lr = linear_regression_model.predict(x_test)

y_pred_train_lasso = lasso_model.predict(x_train)
y_pred_lasso = lasso_model.predict(x_test)

y_pred_train_ridge = ridge_model.predict(x_train)
y_pred_ridge = ridge_model.predict(x_test)

print('Linear Regression')
print('Mean Squared Error:', mean_squared_error(y_test, y_pred_lr))
print('Mean Absolute Error:' , mean_absolute_error(y_test , y_pred_lr))
print('R2 Train Score:', r2_score(y_train, y_pred_train_lr))
print('R2 Test Score:', r2_score(y_test, y_pred_lr))

print('Lasso Regularization')
print('Mean Squared Error:', mean_squared_error(y_test, y_pred_lasso))
print('Mean Absolute Error:' , mean_absolute_error(y_test , y_pred_lasso))
print('R2 Train Score:', r2_score(y_train, y_pred_train_lasso))
print('R2 Test Score:', r2_score(y_test, y_pred_lasso))

print('Ridge Regularization')
print('Mean Squared Error:', mean_squared_error(y_test, y_pred_ridge))
print('Mean Absolute Error:' , mean_absolute_error(y_test , y_pred_ridge))
print('R2 Train Score:', r2_score(y_train, y_pred_train_ridge))
print('R2 Test Score:', r2_score(y_test, y_pred_ridge))